# ddprof

Native profiler from Datadog

# Quick Start

Download the latest ddprof binary from http://binaries.ddbuild.io/ddprof/release/ddprof.  Note that you will need to make the resulting object executable (e.g., `chmod +x ./ddprof`).

Refer to docs/Commands.md for the commands supported by `ddprof`.  This document is autogenerated, so it should be contemporary to the release.

`ddprof` is a wrapper, so using it should be as simple as injecting the binary into your container and wrapping your `run.sh` (or whatever) in it.  `ddprof` will use environment variables if they are available, overriding them with commandline parameters if given.  The `event` interface is fully functional, but at the time of writing the profiling backend is not configured to accept these new profile types.  Let me know if there's anything particularly interesting you'd like to instrument.
```
./ddprof -S my_native_service ./run.sh
```

# Growing Pains

This is a list of things we don't have fully operational yet, which may impact onboarding or quality-of-life.

This is a *pre-beta* release.  It should not be destructive, but it may be useless.  Still--please only deploy in production if you have a very high risk tolerance, a great reversion strategy, and you've taped your pager to your face.  If you do it anyway, please don't @ me in your postmortem :)

* Profiling backend does not currently colorize flamegraphs according to code source
* `ddprof` does not support framepointers
* `ddprof` does not support split debuginfo
* `ddprof` does not furnish overhead numbers to end users
* `ddprof` does not yet implement retry if intake is unreachable

# Overview

*ddprof* is a commandline utility for engaging kernel-mediated telemetry of an application and forwarding the resulting information to the Datadog backend.  In several ways, it's similar to the `perf record` tool.  Currently, *ddprof* is limited to CPU profiling of ELF binaries equipped with debuginfo.


# Key Features

## Overhead

Full analysis of *ddprof* overhead is pending; but users of CPU-time profiling can rely on the following observations.  When there is sufficient computational headroom on the instance for *ddprof* to remain uncompetetive with the target workload (in other words, if the kernel CPU scheduler doesn't need to time-slice the target application against *ddprof*), expect *ddprof* to add less than 1% latency--usually less than 0.1%.  The precise definition of "enough" is workload dependent, but effort will be made to provide rule-of-thumb estimates once analysis has completed.


## Safety

Unlike runtime profilers, the native profiler requires no code modifications of the target service.  It doesn't direct signals at the target, use any `LD_PRELOAD` tricks, replace shared objects, or otherwise interfere with program execution at the process level once the target application has been launched. 

In particular:
* While segfaults and deadlocks can interrupt profiling, they do not propagate to the target application.  A future commit will offer auto-restart options for such cases.
* PID wrapper returns the PID of the target, rather than the PID of `ddprof`.  This is great when you are already running your target under a wrapper or if you're trying to wrap the init process of a PID namespace (as might be the case for containers).


# Docs

Architectural showpieces and such will always be available in the `docs/` folder.


# Prerequisites

In order to take advantage of *ddprof*, you need a few things

* Linux kernel 4.15 or later (if you need to support an earlier kernel, create an issue outlining your need!  If you're blocked in a *libc* issue, also create an issue and we'll resolve it even sooner)
* Your desired application or libraries must have debuginfo.  This means they either have a `.eh_frame` or `.debug_info`.  *ddprof* will, but does not currently, support split debuginfo.
* Access to `perf events`.  See below.


## seccomp

By default, *seccomp* disables the `perf_event_open()` API.  You'll need to make sure you can access it.

## perf_event_paranoid

CPU profiling is available even with the strictest `perf_event_paranoid` mode offered by the Linux kernel--*ddprof* registers self-instrumentation for a process, which is always allowed (a process can look at its own stack and registers), then it steps out of the way.  However, it is possible that something like SELinux or AppArmor implements a further line of defense.  Detecting such configurations is currently outside of the scope of this document, but will be provided eventually.

Unfortunately, due to the rich and storied history of the perf events subsystem, (read:  it's been the originator of security bugs), some distros are shipped with a kernel patch that offers a `perf_event_paranoid == 3` configuration, which shuts down access to the interface (unless a process has `CAP_SYS_ADMIN` or possibly `CAP_PERFMON` on suitably recent kernels).  To use *ddprof* in such a scenario, you're going to have to run at a higher level of permission--either `CAP_SYS_ADMIN` (you can set a capability directly in Docker or through the container`securityContext`/`capabilities` in K8s) will have to be set or you'll need to figure out how to lower the `perf_event_paranoid` sysctl.


### Forward Considerations

In 2016, Debian and Android began running a kernel [patch](https://patchwork.kernel.org/project/kernel-hardening/patch/1469630746-32279-1-git-send-email-jeffv@google.com/) which implemented a new `perf_event_paranoid == 3` sysctl setting, which would totally disable the `perf_event_open()` syscall for processes without `CAP_SYS_ADMIN`.  This patch was contemporary with the emergence of at least four CVE (security issues) stemming from the `perf_event_open()` syscall (mostly around privileged data access).

This patch was rejected in fairly strong terms.  There were a few different themes in the thread.  Locking system-wide disablement behind `CAP_SYS_ADMIN` was seen as too big a hammer; on a system with the interface disabled, the only way to enable it is via a capability which widens the attack surface substantially (in kernel 5.4, there are hundreds of checks for `CAP_SYS_ADMIN`--which is also known as "root 2.0").  Moreover, from the perspective of kernel development, it's problematic to admit that an interface is unsafe and should ever be disabled when the underlying subsystem can't be removed; it's more scalable to allow the subsystem to be removed at build-time or subject such components to the testing necessary to ensure they are safe.

Given the historical status of `perf_event_open()`, one valid concern is, how safe is it to run `perf_event_paranoid == 2` in prod?  This is difficult to qualify fully, but one category of insights comes from the rate at which long-running targeted fuzzing campaigns succeed in finding new bugs.  There has been some [excellent work](http://web.eece.maine.edu/~vweaver/projects/perf_events/fuzzer/bugs_found.html) in this regard (see [here](http://web.eece.maine.edu/~vweaver/projects/perf_events/fuzzer/2019_perf_fuzzer_tr.pdf) as well).  The rate of serious issues and security-related bugs has decreased substantially during the v4 kernel series, and those issues have become increasingly specific.

With the v5.8 kernel, a [new capability](https://lwn.net/Articles/812719/]=) was added for granting heightened access to `perf_event_open()`.  Morevoer, [hooks](https://github.com/torvalds/linux/commit/da97e18458fb42d7c00fac5fd1c56a3896ec666e) for LSM have been added in v5.12, which will allow administrators even more granular control over the consumption of the subsystem.  Unfortunately, mainline LTS distros are barely using v5.4 at the time of writing, so it will be some time before either of these improvements has been fully standardized.  The question remains--in the tradeoff between security and observability, on a contemporary distro, when does it make sense to run `perf_event_paranoid < 3`?

My opinion?  If you're running a kernel dated from late 2017 or later, just enable `perf_events`.


#### Can we do better?

Possibly.

##### Capability Downgrade

One idea would be to offer a new commandline argument to *ddprof*.  When this argument is set, detect `CAP_SYS_ADMIN` on startup, perform the instrumentation, then *drop* the capability in both the profiling daemon and the target application.  This will ensure that the instrumentation can be enabled, but denies attackers the benefit of using it directly.

##### Alternative timing mode

For a variety of reasons, we thought of launching with `perf_event_open()`.  We could also measure time using the standard `set_itimer()` approach.  There are a few unfortunate consequences to this:
 * itimers are mediated through Unix signals, which steal execution from the instrumented process (adds latency)
 * signals have more skid than the kernel code, sometimes by a truly significant margin
 * signals can interrupt syscalls, which can break client code
 * signals don't follow forks
 * have to implement new message passing system to bring samples up from children
 * signal delivery is non-uniform through a thread pool--this isn't an academic point, sampling hugely favors the earliest-spawned thread
 * users can over-write signal handlers

Some of this can be controlled for by implementing an LD_PRELOAD-type trick inside of a wrapper, which could catch `fork()` calls into libc and implement some other niceties, but I'm not sure how much effort this will be to support both glibc/musl across the major versions we have to support.

##### Sidecar Mode

In this distribution, the customer would install `ddprof` as a daemon in a sidecar container.  We'd have to implement a new mode which will watch all processes on all CPUs, which has some issues for finding mappings and debug symbols (but nothing major).  I don't anticipate this to add more latency, but it may erode performance by adding a lot of noise, which will add garbage to our unwinding caches.


# Things we don't know

* For a machine with many active processes, is there more overhead when we are instrumenting a minority or when we instrument a majority (from a single invocation)?
* When the machine is non-saturated, is the distribution of latency uniform over time (or perhaps some other measurable)?
