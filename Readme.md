# ddprof

Native profiler from Datadog

## Quick Start

Download the latest ddprof binary from [ddprof](http://binaries.ddbuild.io/ddprof/release/ddprof).  Note that you will need to make the resulting object executable (e.g., `chmod +x ./ddprof`).

Refer to docs/Commands.md for the commands supported by `ddprof`.  This document is autogenerated, so it should be contemporary to the release.

`ddprof` is a wrapper, so using it should be as simple as injecting the binary into your container and wrapping your `run.sh` (or whatever) in it.  `ddprof` will use environment variables if they are available, overriding them with commandline parameters if given.  The `event` interface is fully functional, but at the time of writing the profiling backend is not configured to accept these new profile types.  Let me know if there's anything particularly interesting you'd like to instrument.

```bash
./ddprof -S my_native_service ./run.sh
```

## Growing Pains

This is a list of things we don't have fully operational yet, which may impact onboarding or quality-of-life.

This is a *pre-beta* release.  It should not be destructive, but it may be useless.  Still--e only deploy in production if you have a very high risk tolerance, a great reversion strategy, and you've taped your pager to your face.  If you do it anyway, please don't @ me in your postmortem :)

* Profiling backend does not currently colorize flamegraphs according to code source
* `ddprof` does not support framepointers
* `ddprof` does not support split debuginfo
* `ddprof` does not furnish overhead numbers to end users
* `ddprof` does not yet implement retry if intake is unreachable

## Overview

*ddprof* is a commandline utility for engaging kernel-mediated telemetry of an application and forwarding the resulting information to the Datadog backend.  In several ways, it's similar to the `perf record` tool.  Currently, *ddprof* is limited to CPU profiling of ELF binaries equipped with debuginfo.

## Key Features

### Overhead

Full analysis of *ddprof* overhead is pending; but users of CPU-time profiling can rely on the following observations.  When there is sufficient computational headroom on the instance for *ddprof* to remain uncompetetive with the target workload (in other words, if the kernel CPU scheduler doesn't need to time-slice the target application against *ddprof*), expect *ddprof* to add less than 1% latency--usually less than 0.1%.  The precise definition of "enough" is workload dependent, but effort will be made to provide rule-of-thumb estimates once analysis has completed.

### Safety

Unlike runtime profilers, the native profiler requires no code modifications of the target service.  It doesn't direct signals at the target, use any `LD_PRELOAD` tricks, replace shared objects, or otherwise interfere with program execution at the process level once the target application has been launched.

In particular:

* While segfaults and deadlocks can interrupt profiling, they do not propagate to the target application.  A future commit will offer auto-restart options for such cases.
* PID wrapper returns the PID of the target, rather than the PID of `ddprof`.  This is great when you are already running your target under a wrapper or if you're trying to wrap the init process of a PID namespace (as might be the case for containers).

## Docs

Architectural showpieces and such will always be available in the `docs/` folder.

## Prerequisites

In order to take advantage of *ddprof*, you need a few things

* Linux kernel 4.15 or later (if you need to support an earlier kernel, create an issue outlining your need!  If you're blocked in a *libc* issue, also create an issue and we'll resolve it even sooner)
* Your desired application or libraries must have debuginfo.  This means they either have a `.eh_frame` or `.debug_info`.  *ddprof* will, but does not currently, support split debuginfo.
* Access to `perf events`.  See below.

### seccomp

By default, *seccomp* disables the `perf_event_open()` API.  You'll need to make sure you can access it.

### perf_event_paranoid

CPU profiling is available even with the strictest `perf_event_paranoid` mode offered by the Linux kernel.  Unfortunately, some distros (notably, Ubuntu) take it a step further.  See docs/PerfEventParanoid.md for a more complete discussion of this topic.

#### Can we do better?

Possibly.

##### Capability Downgrade

One idea would be to offer a new commandline argument to *ddprof*.  When this argument is set, detect `CAP_SYS_ADMIN` on startup, perform the instrumentation, then *drop* the capability in both the profiling daemon and the target application.  This will ensure that the instrumentation can be enabled, but denies attackers the benefit of using it directly.

##### Alternative timing mode

For a variety of reasons, we thought of launching with `perf_event_open()`.  We could also measure time using the standard `set_itimer()` approach.  There are a few unfortunate consequences to this:

* itimers are mediated through Unix signals, which steal execution from the instrumented process (adds latency)
* signals have more skid than the kernel code, sometimes by a truly significant margin
* signals can interrupt syscalls, which can break client code
* signals don't follow forks
* have to implement new message passing system to bring samples up from children
* signal delivery is non-uniform through a thread pool--this isn't an academic point, sampling hugely favors the earliest-spawned thread
* users can over-write signal handlers

Some of this can be controlled for by implementing an LD_PRELOAD-type trick inside of a wrapper, which could catch `fork()` calls into libc and implement some other niceties, but I'm not sure how much effort this will be to support both glibc/musl across the major versions we have to support.

##### Sidecar Mode

In this distribution, the customer would install `ddprof` as a daemon in a sidecar container.  We'd have to implement a new mode which will watch all processes on all CPUs, which has some issues for finding mappings and debug symbols (but nothing major).  I don't anticipate this to add more latency, but it may erode performance by adding a lot of noise, which will add garbage to our unwinding caches.

## Things we don't know

* For a machine with many active processes, is there more overhead when we are instrumenting a minority or when we instrument a majority (from a single invocation)?
* When the machine is non-saturated, is the distribution of latency uniform over time (or perhaps some other measurable)?

## Building from source

[Build](./docs/Build.md)
